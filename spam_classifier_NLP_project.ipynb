{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749f9d43",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8008b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371a55d",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1037c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_table(\"SMSSpamCollection\", header=None, encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af01ea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2358a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "classes = df[0]\n",
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee316c",
   "metadata": {},
   "source": [
    "### Pre-processing of data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c830bffb",
   "metadata": {},
   "source": [
    "Preprocessing the data is an essential step in natural language process. In the following cells, we will convert our class labels to binary values using the LabelEncoder from sklearn, replace email addresses, URLs, phone numbers, and other symbols by using regular expressions, remove stop words, and extract word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56dc04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# converting class label to  binary values,0=ham,1=spam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "Y=encoder.fit_transform(classes)\n",
    "\n",
    "print(Y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fba4b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Go until jurong point, crazy.. Available only ...\n",
      "1                         Ok lar... Joking wif u oni...\n",
      "2     Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3     U dun say so early hor... U c already then say...\n",
      "4     Nah I don't think he goes to usf, he lives aro...\n",
      "5     FreeMsg Hey there darling it's been 3 week's n...\n",
      "6     Even my brother is not like to speak with me. ...\n",
      "7     As per your request 'Melle Melle (Oru Minnamin...\n",
      "8     WINNER!! As a valued network customer you have...\n",
      "9     Had your mobile 11 months or more? U R entitle...\n",
      "10    I'm gonna be home soon and i don't want to tal...\n",
      "11    SIX chances to win CASH! From 100 to 20,000 po...\n",
      "12    URGENT! You have won a 1 week FREE membership ...\n",
      "13    I've been searching for the right words to tha...\n",
      "14                  I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# now store SMS message data\n",
    "text_messages= df[1]\n",
    "\n",
    "print(text_messages[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expressions\n",
    "\n",
    "we want to switch some things like if have email address in text messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5df21bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace email address with \"email\"\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\"emailaddress\")\n",
    "\n",
    "# replae URL with \"webaddress\"\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2-3}(/\\S*)?$',\"webaddress\")\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "processed = processed.str.replace(r'£|\\$', 'moneysymb')\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr')\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n",
    "\n",
    "# Remove punctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2342826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point crazy available only in ...\n",
       "1                                 ok lar joking wif u oni\n",
       "2       free entry in numbr a wkly comp to win fa cup ...\n",
       "3             u dun say so early hor u c already then say\n",
       "4       nah i don t think he goes to usf he lives arou...\n",
       "                              ...                        \n",
       "5567    this is the numbrnd time we have tried numbr c...\n",
       "5568                  will ü b going to esplanade fr home\n",
       "5569    pity was in mood for that so any other suggest...\n",
       "5570    the guy did some bitching but i acted like i d...\n",
       "5571                            rofl its true to its name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change word with lower case \n",
    "processed = processed.str.lower()\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed2f3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the text\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e01ba2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords  = set(stopwords.words(\"english\"))\n",
    "\n",
    "processed = processed.apply(lambda x : \" \".join(i for i in x.split() if i not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f0c9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point crazy available bugis n great ...\n",
       "1                                 ok lar joking wif u oni\n",
       "2       free entry numbr wkly comp win fa cup final tk...\n",
       "3                     u dun say early hor u c already say\n",
       "4                  nah think goes usf lives around though\n",
       "                              ...                        \n",
       "5567    numbrnd time tried numbr contact u u moneysymb...\n",
       "5568                          ü b going esplanade fr home\n",
       "5569                                pity mood suggestions\n",
       "5570    guy bitching acted like interested buying some...\n",
       "5571                                       rofl true name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fc2ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use stemming\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x : \" \".join(ps.stem(i) for i in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8779110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     go jurong point crazi avail bugi n great world...\n",
       "1                                 ok lar joke wif u oni\n",
       "2     free entri numbr wkli comp win fa cup final tk...\n",
       "3                   u dun say earli hor u c alreadi say\n",
       "4                  nah think goe usf live around though\n",
       "5     freemsg hey darl numbr week word back like fun...\n",
       "6         even brother like speak treat like aid patent\n",
       "7     per request mell mell oru minnaminungint nurun...\n",
       "8     winner valu network custom select receivea mon...\n",
       "9     mobil numbr month u r entitl updat latest colo...\n",
       "10    gonna home soon want talk stuff anymor tonight...\n",
       "11    six chanc win cash numbr numbr numbr pound txt...\n",
       "12    urgent numbr week free membership moneysymbnum...\n",
       "13    search right word thank breather promis wont t...\n",
       "14                                          date sunday\n",
       "15    xxxmobilemovieclub use credit click wap link n...\n",
       "16                                           oh k watch\n",
       "17    eh u rememb numbr spell name ye v naughti make...\n",
       "18                           fine way u feel way gota b\n",
       "19    england v macedonia dont miss goal team news t...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e3ef0",
   "metadata": {},
   "source": [
    "### Feature generating"
   ]
  },
  {
   "cell_type": "raw",
   "id": "160e2b90",
   "metadata": {},
   "source": [
    "Feature engineering is the process of using domain knowledge of the data to create features for machine learning algorithms. In this project, the words in each text message will be our features. For this purpose, it will be necessary to tokenize each word. We will use the 1500 most common words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b037874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# creating bag of words\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "all_words= nltk.FreqDist(all_words)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da3d51db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words 6579\n",
      "Most common words [('numbr', 2648), ('u', 1207), ('call', 674), ('go', 456), ('get', 451), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 304), ('moneysymbnumbr', 303), ('ok', 293), ('free', 284), ('day', 276), ('know', 275), ('love', 266), ('like', 261), ('got', 252), ('time', 252), ('good', 248), ('want', 247)]\n"
     ]
    }
   ],
   "source": [
    "# print total number of words and the 15 most common words\n",
    "\n",
    "print(\"Number of Words\",len(all_words))\n",
    "print(\"Most common words\",all_words.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddf3d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use most common 1500 words as features\n",
    "word_features = list(all_words.keys())[:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20bd28ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94a38398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'bugi',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat',\n",
       " 'ok',\n",
       " 'lar',\n",
       " 'joke',\n",
       " 'wif',\n",
       " 'u',\n",
       " 'oni',\n",
       " 'free',\n",
       " 'entri',\n",
       " 'numbr',\n",
       " 'wkli',\n",
       " 'comp',\n",
       " 'win',\n",
       " 'fa',\n",
       " 'cup',\n",
       " 'final',\n",
       " 'tkt',\n",
       " 'numbrst',\n",
       " 'may',\n",
       " 'text',\n",
       " 'receiv',\n",
       " 'question',\n",
       " 'std',\n",
       " 'txt',\n",
       " 'rate',\n",
       " 'c',\n",
       " 'appli',\n",
       " 'numbrovernumbr',\n",
       " 'dun',\n",
       " 'say',\n",
       " 'earli',\n",
       " 'hor',\n",
       " 'alreadi',\n",
       " 'nah',\n",
       " 'think',\n",
       " 'goe',\n",
       " 'usf',\n",
       " 'live',\n",
       " 'around',\n",
       " 'though',\n",
       " 'freemsg',\n",
       " 'hey',\n",
       " 'darl',\n",
       " 'week',\n",
       " 'word',\n",
       " 'back',\n",
       " 'like',\n",
       " 'fun',\n",
       " 'still',\n",
       " 'tb',\n",
       " 'xxx',\n",
       " 'chg',\n",
       " 'send',\n",
       " 'moneysymbnumbr',\n",
       " 'rcv',\n",
       " 'even',\n",
       " 'brother',\n",
       " 'speak',\n",
       " 'treat',\n",
       " 'aid',\n",
       " 'patent',\n",
       " 'per',\n",
       " 'request',\n",
       " 'mell',\n",
       " 'oru',\n",
       " 'minnaminungint',\n",
       " 'nurungu',\n",
       " 'vettam',\n",
       " 'set',\n",
       " 'callertun',\n",
       " 'caller',\n",
       " 'press',\n",
       " 'copi',\n",
       " 'friend',\n",
       " 'winner',\n",
       " 'valu',\n",
       " 'network',\n",
       " 'custom',\n",
       " 'select',\n",
       " 'receivea',\n",
       " 'prize',\n",
       " 'reward',\n",
       " 'claim',\n",
       " 'call',\n",
       " 'code',\n",
       " 'klnumbr',\n",
       " 'valid',\n",
       " 'hour',\n",
       " 'mobil',\n",
       " 'month',\n",
       " 'r',\n",
       " 'entitl',\n",
       " 'updat',\n",
       " 'latest',\n",
       " 'colour',\n",
       " 'camera',\n",
       " 'co',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'home',\n",
       " 'soon',\n",
       " 'want',\n",
       " 'talk',\n",
       " 'stuff',\n",
       " 'anymor',\n",
       " 'tonight',\n",
       " 'k',\n",
       " 'cri',\n",
       " 'enough',\n",
       " 'today',\n",
       " 'six',\n",
       " 'chanc',\n",
       " 'cash',\n",
       " 'pound',\n",
       " 'cshnumbr',\n",
       " 'cost',\n",
       " 'numbrp',\n",
       " 'day',\n",
       " 'numbrday',\n",
       " 'tsandc',\n",
       " 'repli',\n",
       " 'hl',\n",
       " 'info',\n",
       " 'urgent',\n",
       " 'membership',\n",
       " 'jackpot',\n",
       " 'www',\n",
       " 'dbuk',\n",
       " 'net',\n",
       " 'lccltd',\n",
       " 'pobox',\n",
       " 'numbrldnwnumbranumbrrwnumbr',\n",
       " 'search',\n",
       " 'right',\n",
       " 'thank',\n",
       " 'breather',\n",
       " 'promis',\n",
       " 'wont',\n",
       " 'take',\n",
       " 'help',\n",
       " 'grant',\n",
       " 'fulfil',\n",
       " 'wonder',\n",
       " 'bless',\n",
       " 'time',\n",
       " 'date',\n",
       " 'sunday',\n",
       " 'xxxmobilemovieclub',\n",
       " 'use',\n",
       " 'credit',\n",
       " 'click',\n",
       " 'wap',\n",
       " 'link',\n",
       " 'next',\n",
       " 'messag',\n",
       " 'http',\n",
       " 'com',\n",
       " 'qjkgighjjgcbl',\n",
       " 'oh',\n",
       " 'watch',\n",
       " 'eh',\n",
       " 'rememb',\n",
       " 'spell',\n",
       " 'name',\n",
       " 'ye',\n",
       " 'v',\n",
       " 'naughti',\n",
       " 'make',\n",
       " 'wet',\n",
       " 'fine',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'gota',\n",
       " 'b',\n",
       " 'england',\n",
       " 'macedonia',\n",
       " 'dont',\n",
       " 'miss',\n",
       " 'goal',\n",
       " 'team',\n",
       " 'news',\n",
       " 'ur',\n",
       " 'nation',\n",
       " 'eg',\n",
       " 'tri',\n",
       " 'wale',\n",
       " 'scotland',\n",
       " 'numbrtxt',\n",
       " 'únumbr',\n",
       " 'poboxoxnumbrwnumbrwq',\n",
       " 'serious',\n",
       " 'ha',\n",
       " 'ü',\n",
       " 'pay',\n",
       " 'first',\n",
       " 'da',\n",
       " 'stock',\n",
       " 'comin',\n",
       " 'aft',\n",
       " 'finish',\n",
       " 'lunch',\n",
       " 'str',\n",
       " 'lor',\n",
       " 'ard',\n",
       " 'smth',\n",
       " 'ffffffffff',\n",
       " 'alright',\n",
       " 'meet',\n",
       " 'sooner',\n",
       " 'forc',\n",
       " 'eat',\n",
       " 'slice',\n",
       " 'realli',\n",
       " 'hungri',\n",
       " 'tho',\n",
       " 'suck',\n",
       " 'mark',\n",
       " 'get',\n",
       " 'worri',\n",
       " 'know',\n",
       " 'sick',\n",
       " 'turn',\n",
       " 'pizza',\n",
       " 'lol',\n",
       " 'alway',\n",
       " 'convinc',\n",
       " 'catch',\n",
       " 'bu',\n",
       " 'fri',\n",
       " 'egg',\n",
       " 'tea',\n",
       " 'mom',\n",
       " 'left',\n",
       " 'dinner',\n",
       " 'love',\n",
       " 'amp',\n",
       " 'pack',\n",
       " 'car',\n",
       " 'let',\n",
       " 'room',\n",
       " 'ahhh',\n",
       " 'work',\n",
       " 'vagu',\n",
       " 'wait',\n",
       " 'clear',\n",
       " 'sure',\n",
       " 'sarcast',\n",
       " 'x',\n",
       " 'us',\n",
       " 'yeah',\n",
       " 'apologet',\n",
       " 'fallen',\n",
       " 'actin',\n",
       " 'spoilt',\n",
       " 'child',\n",
       " 'caught',\n",
       " 'till',\n",
       " 'badli',\n",
       " 'cheer',\n",
       " 'tell',\n",
       " 'anyth',\n",
       " 'fear',\n",
       " 'faint',\n",
       " 'housework',\n",
       " 'quick',\n",
       " 'cuppa',\n",
       " 'subscript',\n",
       " 'rington',\n",
       " 'uk',\n",
       " 'charg',\n",
       " 'pleas',\n",
       " 'confirm',\n",
       " 'yup',\n",
       " 'look',\n",
       " 'msg',\n",
       " 'xuhui',\n",
       " 'learn',\n",
       " 'numbrnd',\n",
       " 'lesson',\n",
       " 'numbram',\n",
       " 'oop',\n",
       " 'roommat',\n",
       " 'done',\n",
       " 'see',\n",
       " 'letter',\n",
       " 'decid',\n",
       " 'hello',\n",
       " 'saturday',\n",
       " 'tomo',\n",
       " 'invit',\n",
       " 'pl',\n",
       " 'ahead',\n",
       " 'watt',\n",
       " 'weekend',\n",
       " 'abiola',\n",
       " 'forget',\n",
       " 'need',\n",
       " 'crave',\n",
       " 'sweet',\n",
       " 'arabian',\n",
       " 'steed',\n",
       " 'mmmmmm',\n",
       " 'yummi',\n",
       " 'rodger',\n",
       " 'burn',\n",
       " 'sm',\n",
       " 'nokia',\n",
       " 'camcord',\n",
       " 'deliveri',\n",
       " 'tomorrow',\n",
       " 'hope',\n",
       " 'man',\n",
       " 'well',\n",
       " 'endow',\n",
       " 'lt',\n",
       " 'gt',\n",
       " 'inch',\n",
       " 'hep',\n",
       " 'immunis',\n",
       " 'nigeria',\n",
       " 'fair',\n",
       " 'tyler',\n",
       " 'could',\n",
       " 'mayb',\n",
       " 'ask',\n",
       " 'bit',\n",
       " 'stubborn',\n",
       " 'hospit',\n",
       " 'kept',\n",
       " 'weak',\n",
       " 'sucker',\n",
       " 'saw',\n",
       " 'class',\n",
       " 'gram',\n",
       " 'usual',\n",
       " 'run',\n",
       " 'half',\n",
       " 'eighth',\n",
       " 'smarter',\n",
       " 'almost',\n",
       " 'whole',\n",
       " 'second',\n",
       " 'fyi',\n",
       " 'ride',\n",
       " 'morn',\n",
       " 'crash',\n",
       " 'place',\n",
       " 'wow',\n",
       " 'never',\n",
       " 'realiz',\n",
       " 'embarass',\n",
       " 'accomod',\n",
       " 'thought',\n",
       " 'sinc',\n",
       " 'best',\n",
       " 'seem',\n",
       " 'happi',\n",
       " 'cave',\n",
       " 'sorri',\n",
       " 'give',\n",
       " 'offer',\n",
       " 'ac',\n",
       " 'sptv',\n",
       " 'new',\n",
       " 'jersey',\n",
       " 'devil',\n",
       " 'detroit',\n",
       " 'red',\n",
       " 'wing',\n",
       " 'play',\n",
       " 'ice',\n",
       " 'hockey',\n",
       " 'correct',\n",
       " 'incorrect',\n",
       " 'end',\n",
       " 'mallika',\n",
       " 'sherawat',\n",
       " 'yesterday',\n",
       " 'find',\n",
       " 'url',\n",
       " 'congrat',\n",
       " 'year',\n",
       " 'special',\n",
       " 'cinema',\n",
       " 'pass',\n",
       " 'suprman',\n",
       " 'matrixnumbr',\n",
       " 'starwarsnumbr',\n",
       " 'etc',\n",
       " 'bxnumbr',\n",
       " 'ipnumbr',\n",
       " 'numbrw',\n",
       " 'numbrpm',\n",
       " 'later',\n",
       " 'reach',\n",
       " 'gauti',\n",
       " 'sehwag',\n",
       " 'odi',\n",
       " 'seri',\n",
       " 'pick',\n",
       " 'burger',\n",
       " 'move',\n",
       " 'pain',\n",
       " 'kill',\n",
       " 'good',\n",
       " 'girl',\n",
       " 'situat',\n",
       " 'seeker',\n",
       " 'part',\n",
       " 'check',\n",
       " 'iq',\n",
       " 'took',\n",
       " 'forev',\n",
       " 'come',\n",
       " 'doubl',\n",
       " 'hair',\n",
       " 'dresser',\n",
       " 'said',\n",
       " 'wun',\n",
       " 'cut',\n",
       " 'short',\n",
       " 'nice',\n",
       " 'advis',\n",
       " 'follow',\n",
       " 'recent',\n",
       " 'review',\n",
       " 'mob',\n",
       " 'award',\n",
       " 'bonu',\n",
       " 'song',\n",
       " 'dedic',\n",
       " 'valuabl',\n",
       " 'frnd',\n",
       " 'rpli',\n",
       " 'complimentari',\n",
       " 'trip',\n",
       " 'eurodisinc',\n",
       " 'trav',\n",
       " 'aco',\n",
       " 'entrynumbr',\n",
       " 'di',\n",
       " 'morefrmmob',\n",
       " 'shracomorsglsuplt',\n",
       " 'lsnumbr',\n",
       " 'numbraj',\n",
       " 'hear',\n",
       " 'divorc',\n",
       " 'barbi',\n",
       " 'ken',\n",
       " 'plane',\n",
       " 'wah',\n",
       " 'lucki',\n",
       " 'save',\n",
       " 'money',\n",
       " 'hee',\n",
       " 'hi',\n",
       " 'babe',\n",
       " 'im',\n",
       " 'wan',\n",
       " 'someth',\n",
       " 'xx',\n",
       " 'perform',\n",
       " 'machan',\n",
       " 'that',\n",
       " 'cool',\n",
       " 'gentleman',\n",
       " 'digniti',\n",
       " 'respect',\n",
       " 'peopl',\n",
       " 'much',\n",
       " 'shi',\n",
       " 'pa',\n",
       " 'oper',\n",
       " 'job',\n",
       " 'ta',\n",
       " 'earn',\n",
       " 'ah',\n",
       " 'stop',\n",
       " 'urgnt',\n",
       " 'real',\n",
       " 'yo',\n",
       " 'ticket',\n",
       " 'one',\n",
       " 'jacket',\n",
       " 'multi',\n",
       " 'start',\n",
       " 'came',\n",
       " 'bed',\n",
       " 'coin',\n",
       " 'factori',\n",
       " 'nitro',\n",
       " 'ela',\n",
       " 'kano',\n",
       " 'il',\n",
       " 'download',\n",
       " 'wen',\n",
       " 'stand',\n",
       " 'close',\n",
       " 'anoth',\n",
       " 'night',\n",
       " 'spent',\n",
       " 'late',\n",
       " 'afternoon',\n",
       " 'casualti',\n",
       " 'mean',\n",
       " 'stuffnumbrmoro',\n",
       " 'includ',\n",
       " 'sheet',\n",
       " 'smile',\n",
       " 'pleasur',\n",
       " 'troubl',\n",
       " 'pour',\n",
       " 'rain',\n",
       " 'sumnumbr',\n",
       " 'hurt',\n",
       " 'becoz',\n",
       " 'someon',\n",
       " 'servic',\n",
       " 'repres',\n",
       " 'guarante',\n",
       " 'havent',\n",
       " 'plan',\n",
       " 'buy',\n",
       " 'lido',\n",
       " 'show',\n",
       " 'collect',\n",
       " 'simpli',\n",
       " 'password',\n",
       " 'mix',\n",
       " 'verifi',\n",
       " 'usher',\n",
       " 'britney',\n",
       " 'fml',\n",
       " 'po',\n",
       " 'box',\n",
       " 'mknumbr',\n",
       " 'numbrh',\n",
       " 'numbrppw',\n",
       " 'telugu',\n",
       " 'movi',\n",
       " 'abt',\n",
       " 'load',\n",
       " 'loan',\n",
       " 'wk',\n",
       " 'hol',\n",
       " 'forgot',\n",
       " 'hairdress',\n",
       " 'appoint',\n",
       " 'four',\n",
       " 'shower',\n",
       " 'beforehand',\n",
       " 'caus',\n",
       " 'prob',\n",
       " 'coffe',\n",
       " 'anim',\n",
       " 'noth',\n",
       " 'els',\n",
       " 'okay',\n",
       " 'price',\n",
       " 'long',\n",
       " 'legal',\n",
       " 'ave',\n",
       " 'am',\n",
       " 'gone',\n",
       " 'numbrth',\n",
       " 'drive',\n",
       " 'test',\n",
       " 'yet',\n",
       " 'guess',\n",
       " 'gave',\n",
       " 'boston',\n",
       " 'men',\n",
       " 'chang',\n",
       " 'locat',\n",
       " 'nyc',\n",
       " 'cuz',\n",
       " 'signin',\n",
       " 'page',\n",
       " 'umma',\n",
       " 'life',\n",
       " 'vava',\n",
       " 'lot',\n",
       " 'dear',\n",
       " 'wish',\n",
       " 'birthday',\n",
       " 'truli',\n",
       " 'memor',\n",
       " 'aight',\n",
       " 'hit',\n",
       " 'would',\n",
       " 'ip',\n",
       " 'address',\n",
       " 'consid',\n",
       " 'comput',\n",
       " 'minecraft',\n",
       " 'server',\n",
       " 'grumpi',\n",
       " 'old',\n",
       " 'better',\n",
       " 'lie',\n",
       " 'busi',\n",
       " 'plural',\n",
       " 'noun',\n",
       " 'research',\n",
       " 'thing',\n",
       " 'scare',\n",
       " 'mah',\n",
       " 'loud',\n",
       " 'gent',\n",
       " 'contact',\n",
       " 'last',\n",
       " 'draw',\n",
       " 'knumbr',\n",
       " 'numbrhr',\n",
       " 'numbrppm',\n",
       " 'wa',\n",
       " 'openin',\n",
       " 'sentenc',\n",
       " 'formal',\n",
       " 'anyway',\n",
       " 'juz',\n",
       " 'tt',\n",
       " 'eatin',\n",
       " 'puttin',\n",
       " 'weight',\n",
       " 'haha',\n",
       " 'anythin',\n",
       " 'happen',\n",
       " 'enter',\n",
       " 'cabin',\n",
       " 'boss',\n",
       " 'felt',\n",
       " 'askd',\n",
       " 'apart',\n",
       " 'went',\n",
       " 'holiday',\n",
       " 'flight',\n",
       " 'inc',\n",
       " 'min',\n",
       " 'goodo',\n",
       " 'must',\n",
       " 'friday',\n",
       " 'potato',\n",
       " 'ratio',\n",
       " 'tortilla',\n",
       " 'hmm',\n",
       " 'uncl',\n",
       " 'inform',\n",
       " 'school',\n",
       " 'directli',\n",
       " 'food',\n",
       " 'privat',\n",
       " 'account',\n",
       " 'statement',\n",
       " 'unredeem',\n",
       " 'identifi',\n",
       " 'expir',\n",
       " 'landlin',\n",
       " 'boxnumbrwrnumbrc',\n",
       " 'appl',\n",
       " 'pair',\n",
       " 'malarki',\n",
       " 'voda',\n",
       " 'number',\n",
       " 'match',\n",
       " 'quot',\n",
       " 'standard',\n",
       " 'app',\n",
       " 'sao',\n",
       " 'mu',\n",
       " 'predict',\n",
       " 'yetund',\n",
       " 'sent',\n",
       " 'bother',\n",
       " 'involv',\n",
       " 'impos',\n",
       " 'apologis',\n",
       " 'del',\n",
       " 'bak',\n",
       " 'sum',\n",
       " 'lucyxx',\n",
       " 'tmorrow',\n",
       " 'answer',\n",
       " 'sunshin',\n",
       " 'quiz',\n",
       " 'q',\n",
       " 'top',\n",
       " 'soni',\n",
       " 'dvd',\n",
       " 'player',\n",
       " 'countri',\n",
       " 'algarv',\n",
       " 'ansr',\n",
       " 'sp',\n",
       " 'tyron',\n",
       " 'laid',\n",
       " 'dog',\n",
       " 'direct',\n",
       " 'join',\n",
       " 'largest',\n",
       " 'bt',\n",
       " 'txting',\n",
       " 'gravel',\n",
       " 'nt',\n",
       " 'ecnumbra',\n",
       " 'emailaddress',\n",
       " 'befor',\n",
       " 'activ',\n",
       " 'chat',\n",
       " 'svc',\n",
       " 'hardcor',\n",
       " 'age',\n",
       " 'yr',\n",
       " 'lazi',\n",
       " 'type',\n",
       " 'lect',\n",
       " 'pouch',\n",
       " 'sir',\n",
       " 'mail',\n",
       " 'swt',\n",
       " 'nver',\n",
       " 'tire',\n",
       " 'littl',\n",
       " 'lovabl',\n",
       " 'person',\n",
       " 'coz',\n",
       " 'somtim',\n",
       " 'occupi',\n",
       " 'biggest',\n",
       " 'heart',\n",
       " 'gud',\n",
       " 'ninumbr',\n",
       " 'open',\n",
       " 'ya',\n",
       " 'dot',\n",
       " 'what',\n",
       " 'staff',\n",
       " 'randi',\n",
       " 'sexi',\n",
       " 'femal',\n",
       " 'local',\n",
       " 'luv',\n",
       " 'netcollex',\n",
       " 'ltd',\n",
       " 'ummma',\n",
       " 'begin',\n",
       " 'qatar',\n",
       " 'pray',\n",
       " 'hard',\n",
       " 'delet',\n",
       " 'sindu',\n",
       " 'birla',\n",
       " 'soft',\n",
       " 'wine',\n",
       " 'flow',\n",
       " 'thk',\n",
       " 'plaza',\n",
       " 'typic',\n",
       " 'everywher',\n",
       " 'dirt',\n",
       " 'floor',\n",
       " 'window',\n",
       " 'shirt',\n",
       " 'sometim',\n",
       " 'mouth',\n",
       " 'dream',\n",
       " 'without',\n",
       " 'chore',\n",
       " 'joy',\n",
       " 'tv',\n",
       " 'exist',\n",
       " 'hail',\n",
       " 'mist',\n",
       " 'becom',\n",
       " 'aaooooright',\n",
       " 'leav',\n",
       " 'hous',\n",
       " 'interview',\n",
       " 'boy',\n",
       " 'annonc',\n",
       " 'arrang',\n",
       " 'keep',\n",
       " 'safe',\n",
       " 'envi',\n",
       " 'everyon',\n",
       " 'parent',\n",
       " 'hand',\n",
       " 'excit',\n",
       " 'spend',\n",
       " 'bootydeli',\n",
       " 'f',\n",
       " 'bangbab',\n",
       " 'order',\n",
       " 'content',\n",
       " 'goto',\n",
       " 'bangb',\n",
       " 'internet',\n",
       " 'menu',\n",
       " 'cultur',\n",
       " 'modul',\n",
       " 'snumbr',\n",
       " 'avoid',\n",
       " 'missunderstd',\n",
       " 'wit',\n",
       " 'belov',\n",
       " 'escap',\n",
       " 'fanci',\n",
       " 'bridg',\n",
       " 'lager',\n",
       " 'complet',\n",
       " 'form',\n",
       " 'clark',\n",
       " 'also',\n",
       " 'utter',\n",
       " 'wast',\n",
       " 'axi',\n",
       " 'bank',\n",
       " 'hmmm',\n",
       " 'hop',\n",
       " 'muz',\n",
       " 'discuss',\n",
       " 'liao',\n",
       " 'bloodi',\n",
       " 'hell',\n",
       " 'cant',\n",
       " 'believ',\n",
       " 'surnam',\n",
       " 'mr',\n",
       " 'ill',\n",
       " 'clue',\n",
       " 'spanish',\n",
       " 'bath',\n",
       " 'carlo',\n",
       " 'mall',\n",
       " 'stay',\n",
       " 'til',\n",
       " 'smoke',\n",
       " 'moneysymb',\n",
       " 'worth',\n",
       " 'doesnt',\n",
       " 'log',\n",
       " 'spoke',\n",
       " 'maneesha',\n",
       " 'satisfi',\n",
       " 'experi',\n",
       " 'toll',\n",
       " 'lift',\n",
       " 'especi',\n",
       " 'approach',\n",
       " 'studi',\n",
       " 'grnumbr',\n",
       " 'trust',\n",
       " 'guy',\n",
       " 'bye',\n",
       " 'handsom',\n",
       " 'toward',\n",
       " 'mummi',\n",
       " 'boytoy',\n",
       " 'awesom',\n",
       " 'minut',\n",
       " 'freephon',\n",
       " 'xma',\n",
       " 'radio',\n",
       " 'ju',\n",
       " 'si',\n",
       " 'uniqu',\n",
       " 'august',\n",
       " 'areyouuniqu',\n",
       " 'leagu',\n",
       " 'touch',\n",
       " 'deal',\n",
       " 'cours',\n",
       " 'howev',\n",
       " 'suggest',\n",
       " 'abl',\n",
       " 'or',\n",
       " 'everi',\n",
       " 'stool',\n",
       " 'settl',\n",
       " 'wishin',\n",
       " 'mrng',\n",
       " 'hav',\n",
       " 'stori',\n",
       " 'hamster',\n",
       " 'dead',\n",
       " 'tmr',\n",
       " 'orchard',\n",
       " 'mrt',\n",
       " 'kate',\n",
       " 'babyjontet',\n",
       " 'found',\n",
       " 'enc',\n",
       " 'buck',\n",
       " 'darlin',\n",
       " 'ive',\n",
       " 'colleg',\n",
       " 'refil',\n",
       " 'success',\n",
       " 'inr',\n",
       " 'decim',\n",
       " 'keralacircl',\n",
       " 'prepaid',\n",
       " 'balanc',\n",
       " 'rs',\n",
       " 'transact',\n",
       " 'id',\n",
       " 'kr',\n",
       " 'goodmorn',\n",
       " 'sleep',\n",
       " 'ga',\n",
       " 'alter',\n",
       " 'dat',\n",
       " 'ericsson',\n",
       " 'oso',\n",
       " 'can',\n",
       " 'not',\n",
       " 'oredi',\n",
       " 'straight',\n",
       " 'dogg',\n",
       " 'connect',\n",
       " 'refund',\n",
       " 'bill',\n",
       " 'shoot',\n",
       " 'big',\n",
       " 'readi',\n",
       " 'bruv',\n",
       " 'break',\n",
       " 'semest',\n",
       " 'noe',\n",
       " 'leh',\n",
       " 'sound',\n",
       " 'head',\n",
       " 'slept',\n",
       " 'past',\n",
       " 'easi',\n",
       " 'sen',\n",
       " 'exam',\n",
       " 'march',\n",
       " 'atm',\n",
       " 'regist',\n",
       " 'os',\n",
       " 'ubandu',\n",
       " 'instal',\n",
       " 'disk',\n",
       " 'import',\n",
       " 'file',\n",
       " 'system',\n",
       " 'repair',\n",
       " 'shop',\n",
       " 'romant',\n",
       " 'nite',\n",
       " 'sceneri',\n",
       " 'tc',\n",
       " 'biz',\n",
       " 'numbroptout',\n",
       " 'numbrgbp',\n",
       " 'mtmsgnumbr',\n",
       " 'appreci',\n",
       " 'partner',\n",
       " 'career',\n",
       " 'flyng',\n",
       " 'horo',\n",
       " 'star',\n",
       " 'sign',\n",
       " 'g',\n",
       " 'ari',\n",
       " 'compani',\n",
       " 'elama',\n",
       " 'mudyadhu',\n",
       " 'strict',\n",
       " 'teacher',\n",
       " 'bcoz',\n",
       " 'teach',\n",
       " 'conduct',\n",
       " 'gandhipuram',\n",
       " 'walk',\n",
       " 'cross',\n",
       " 'road',\n",
       " 'side',\n",
       " 'street',\n",
       " 'rubber',\n",
       " 'batteri',\n",
       " 'die',\n",
       " 'flirt',\n",
       " 'sam',\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5dc8716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "jurong\n",
      "point\n",
      "crazi\n",
      "avail\n",
      "bugi\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "cine\n",
      "got\n",
      "amor\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "# The find_features function will determine which of the 1500 word features are contained in the review\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Lets see an example!\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3284447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point crazi avail bugi n great world la e buffet cine got amor wat'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "418307a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the messages\n",
    "messages = zip(processed, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37fafee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call find_features function for each SMS message\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc15b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the featureset into trainin and testing datasets using sklearn\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "training,testing = model_selection.train_test_split(featuresets,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f2a8514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n"
     ]
    }
   ],
   "source": [
    "print(len(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4604d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393\n"
     ]
    }
   ],
   "source": [
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6ad05",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "027d27db",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we can start building algorithms! Let's start with a simple linear support vector classifier, then expand to other algorithms. We'll need to import each algorithm we plan on using from sklearn. We also need to import some performance metrics, such as accuracy_score and classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87790fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy 97.84637473079684\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel=\"linear\"))\n",
    "\n",
    "#train the model \n",
    "model.train(training)\n",
    "\n",
    "# test on testing datasets\n",
    "accuracy=nltk.classify.accuracy(model,testing)*100\n",
    "print(\"SVC Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ee8992b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0eb0bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy:92.46231155778895\n",
      "Decision Tree Accuracy:96.91313711414213\n",
      "Random Forest Accuracy:97.63101220387652\n",
      "Logistic Regression Accuracy:97.91816223977028\n",
      "Naive Bayes Accuracy:98.20531227566404\n",
      "SVM Linear Accuracy:97.84637473079684\n"
     ]
    }
   ],
   "source": [
    "# define models to train\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "classifiers = [KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')]\n",
    "\n",
    "models=zip(names,classifiers)\n",
    "\n",
    "for name,model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    \n",
    "    accuracy = nltk.classify.accuracy(nltk_model,testing)*100\n",
    "    print(\"{} Accuracy:{}\".format(name,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7caf4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "895da936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1190\n",
      "           1       0.98      0.87      0.92       203\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.93      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1186</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>26</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1186    4\n",
       "       spam        26  177"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda2b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
